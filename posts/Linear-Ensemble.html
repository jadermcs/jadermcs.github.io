<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Blog - Linear Ensemble</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <link href="https://fonts.googleapis.com/css?family=IBM+Plex+Mono:400,700" rel="stylesheet">
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Blog</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>Linear Ensemble</h1>

            <div class="info">
    Posted on August 17, 2017
    
        by Jader Martins
    
</div>

<h1 id="modelos-regressivos-compostos-para-estimativas-de-preço">Modelos Regressivos Compostos para Estimativas de Preço</h1>
<p>Determinar preços de determinados itens antes de sua entrada no mercado é essencial para boa aceitação e consumo. Disponibilizar um produto no mercado abaixo do preço de mercado não te gera bons retornos, mas também um valor muito alto não agrada aos compradores, modelos regressivos nesse caso são de grande ajuda para a tomada de decisão acerca da precificação de um insumo. A performance preditiva de modelos compostos comparados a modelos simples tem sido notável nas mais diversas áreas<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, modelos simples são aqueles que usam <a href="https://pt.wikipedia.org/wiki/Aprendizado_de_m%C3%A1quina#Abordagens">algoritmos puros do aprendizado de máquina</a>, já modelos compostos combinam as predições de dois ou mais algoritmos na tentativa de melhorar a predição. Nessa postagem buscarei apresentar formas eficientes de combinar modelos para minimizar o erro das predições de preços de metro quadrado de imóveis em Boston.</p>
<h3 id="preparando-os-dados">Preparando os Dados</h3>
<p>Aqui usarei um dataset famoso de preços de casa, mas a técnica aqui abordada pode ser estendida para precificação de quase qualquer coisa. Primeiro importarei e carregarei meu conjunto de dados na variável “boston” utilizando o Pandas, modulo do Python famoso por seus dataframes voltado a analise em finanças. O conjunto de dados advém do módulo Scikit-Learn que usaremos no decorrer desse post para trabalhar com AM, ele forneça ferramentas desde o tratamento dos dados até uma <em>pipeline</em> de aprendizado de máquina. Também usaremos o modulo Numpy.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="op">%</span>matplotlib inline</a>
<a class="sourceLine" id="cb1-2" title="2"></a>
<a class="sourceLine" id="cb1-3" title="3"><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_boston</a>
<a class="sourceLine" id="cb1-4" title="4"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-5" title="5"><span class="im">import</span> pandas <span class="im">as</span> pd</a>
<a class="sourceLine" id="cb1-6" title="6"></a>
<a class="sourceLine" id="cb1-7" title="7">boston <span class="op">=</span> load_boston()</a>
<a class="sourceLine" id="cb1-8" title="8"></a>
<a class="sourceLine" id="cb1-9" title="9">df <span class="op">=</span> pd.DataFrame(</a>
<a class="sourceLine" id="cb1-10" title="10">    np.column_stack([boston.data, boston.target]), </a>
<a class="sourceLine" id="cb1-11" title="11">    columns<span class="op">=</span>np.r_[boston.feature_names, [<span class="st">'MEDV'</span>]])</a>
<a class="sourceLine" id="cb1-12" title="12">df.head()</a></code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
CRIM
</th>
<th>
ZN
</th>
<th>
INDUS
</th>
<th>
CHAS
</th>
<th>
NOX
</th>
<th>
RM
</th>
<th>
AGE
</th>
<th>
DIS
</th>
<th>
RAD
</th>
<th>
TAX
</th>
<th>
PTRATIO
</th>
<th>
B
</th>
<th>
LSTAT
</th>
<th>
MEDV
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0.00632
</td>
<td>
18.0
</td>
<td>
2.31
</td>
<td>
0.0
</td>
<td>
0.538
</td>
<td>
6.575
</td>
<td>
65.2
</td>
<td>
4.0900
</td>
<td>
1.0
</td>
<td>
296.0
</td>
<td>
15.3
</td>
<td>
396.90
</td>
<td>
4.98
</td>
<td>
24.0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
0.02731
</td>
<td>
0.0
</td>
<td>
7.07
</td>
<td>
0.0
</td>
<td>
0.469
</td>
<td>
6.421
</td>
<td>
78.9
</td>
<td>
4.9671
</td>
<td>
2.0
</td>
<td>
242.0
</td>
<td>
17.8
</td>
<td>
396.90
</td>
<td>
9.14
</td>
<td>
21.6
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0.02729
</td>
<td>
0.0
</td>
<td>
7.07
</td>
<td>
0.0
</td>
<td>
0.469
</td>
<td>
7.185
</td>
<td>
61.1
</td>
<td>
4.9671
</td>
<td>
2.0
</td>
<td>
242.0
</td>
<td>
17.8
</td>
<td>
392.83
</td>
<td>
4.03
</td>
<td>
34.7
</td>
</tr>
<tr>
<th>
3
</th>
<td>
0.03237
</td>
<td>
0.0
</td>
<td>
2.18
</td>
<td>
0.0
</td>
<td>
0.458
</td>
<td>
6.998
</td>
<td>
45.8
</td>
<td>
6.0622
</td>
<td>
3.0
</td>
<td>
222.0
</td>
<td>
18.7
</td>
<td>
394.63
</td>
<td>
2.94
</td>
<td>
33.4
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0.06905
</td>
<td>
0.0
</td>
<td>
2.18
</td>
<td>
0.0
</td>
<td>
0.458
</td>
<td>
7.147
</td>
<td>
54.2
</td>
<td>
6.0622
</td>
<td>
3.0
</td>
<td>
222.0
</td>
<td>
18.7
</td>
<td>
396.90
</td>
<td>
5.33
</td>
<td>
36.2
</td>
</tr>
</tbody>
</table>
</div>
<p>Aqui carrego meus dados na variável df e mostro as 5 primeiras linhas com o comando head.</p>
<p>Temos informações como criminalidade da região, idade média da população, etc.. Embora não seja o foco dessa postagem, a distribuição dos nossos dados poderá causar grande dificuldade para nosso regressor modela-la, sendo assim aplicarei uma “feature engineering” simples para tornar nossa distribuição mais normal, em posts futuros será explicado em detalhes o que é feature engineering e como utiliza-la para melhorar suas predições. Primeiro vamos ver como está a distribuição que queremos prever ao lado da distribuição “normalizada” por f.log(x+1), (acrescentar um ao valor nos evita ter problemas com zeros).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="im">import</span> seaborn <span class="im">as</span> sns</a>
<a class="sourceLine" id="cb2-2" title="2"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb2-3" title="3">sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">&quot;whitegrid&quot;</span>, palette<span class="op">=</span><span class="st">&quot;coolwarm&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">df.plot.box(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>), patch_artist<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<p><img src="../images/output_6_1.png" width="600px"></p>
<p>Primeiro carrego as bibliotecas de gráfico que utilizarei no decorrer do texto, defino configurações como estilo e paleta de cores para o gráfico, em seguida monto um dataframe <em>prices</em> para receber duas colunas de valores, uma com o preço sem transformação, outra com o preço tranformado pela função log1p (f.log(x+1)).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">prices <span class="op">=</span> pd.DataFrame({<span class="st">&quot;Preço&quot;</span>:df[<span class="st">&quot;MEDV&quot;</span>], <span class="st">&quot;log(Preço + 1)&quot;</span>:np.log1p(df[<span class="st">&quot;MEDV&quot;</span>])})</a>
<a class="sourceLine" id="cb4-2" title="2"></a>
<a class="sourceLine" id="cb4-3" title="3">prices.hist(color<span class="op">=</span><span class="st">&quot;#F1684E&quot;</span>, bins<span class="op">=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb4-4" title="4">plt.ylabel(<span class="st">&quot;Quantidade&quot;</span>)</a></code></pre></div>
<p><img src="../images/output_8_1.png"></p>
<p>Podemos ver que nossa distribuição ficou menos espaçada e um pouco mais próxima de uma distribuição normal, mas o Python conta com uma função estatística que nos ajuda avaliar se isso será necessário ou não, através do teste de Box-Cox que terá indícios com o grau de Obliquidade (Skewness).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="im">from</span> scipy.stats <span class="im">import</span> skew</a>
<a class="sourceLine" id="cb5-2" title="2"></a>
<a class="sourceLine" id="cb5-3" title="3"><span class="cf">for</span> col <span class="kw">in</span> df.keys():</a>
<a class="sourceLine" id="cb5-4" title="4">    sk <span class="op">=</span> skew(df[col])</a>
<a class="sourceLine" id="cb5-5" title="5">    <span class="cf">if</span> sk <span class="op">&gt;</span> <span class="fl">0.75</span>:</a>
<a class="sourceLine" id="cb5-6" title="6">        <span class="bu">print</span>(col, sk)</a></code></pre></div>
<p>CRIM 5.222039072246122 ZN 2.219063057148425 CHAS 3.395799292642519 DIS 1.0087787565152246 RAD 1.0018334924536951 LSTAT 0.9037707431346133 MEDV 1.104810822864635</p>
<h4 id="um-pouco-de-feature-engeneering">Um Pouco de Feature Engeneering</h4>
<p>O teste de Box-Cox<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> nos diz que um skew acima de 0.75 pode ser linearizado pela função log(x+1), fazendo a distribuição ficar mais normalizada, abaixo disso posso manter o valor como estava sem necessidades de modificação, vamos olhar o antes e depois de aplicar essa função a nossas distribuições. (Suprimi algumas variáveis para não poluir demais o gráfico).</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">dfnorm <span class="op">=</span> (df <span class="op">-</span> df.mean()) <span class="op">/</span> (df.std())</a>
<a class="sourceLine" id="cb6-2" title="2"><span class="cf">for</span> x <span class="kw">in</span> [<span class="st">&quot;CRIM&quot;</span>, <span class="st">&quot;ZN&quot;</span>, <span class="st">&quot;CHAS&quot;</span>,<span class="st">&quot;MEDV&quot;</span>]:</a>
<a class="sourceLine" id="cb6-3" title="3">    sns.kdeplot(dfnorm[x])</a>
<a class="sourceLine" id="cb6-4" title="4">plt.title(<span class="st">&quot;Distribuição Valor Médio&quot;</span>)</a>
<a class="sourceLine" id="cb6-5" title="5">plt.xlabel(<span class="st">&quot;Preço&quot;</span>)</a>
<a class="sourceLine" id="cb6-6" title="6">plt.ylabel(<span class="st">&quot;Quantidade&quot;</span>)</a></code></pre></div>
<p><img src="../images/output_12_1.png"></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1"><span class="cf">for</span> col <span class="kw">in</span> df.keys():</a>
<a class="sourceLine" id="cb7-2" title="2">    sk <span class="op">=</span> skew(df[col])</a>
<a class="sourceLine" id="cb7-3" title="3">    <span class="cf">if</span> sk <span class="op">&gt;</span> <span class="fl">0.75</span>:</a>
<a class="sourceLine" id="cb7-4" title="4">        df[col] <span class="op">=</span> np.log1p(df[col])</a></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" title="1">dfnorm <span class="op">=</span> (df <span class="op">-</span> df.mean()) <span class="op">/</span> (df.std())</a>
<a class="sourceLine" id="cb8-2" title="2"><span class="cf">for</span> x <span class="kw">in</span> [<span class="st">&quot;CRIM&quot;</span>, <span class="st">&quot;ZN&quot;</span>, <span class="st">&quot;CHAS&quot;</span>,<span class="st">&quot;MEDV&quot;</span>]:</a>
<a class="sourceLine" id="cb8-3" title="3">    sns.kdeplot(dfnorm[x])</a>
<a class="sourceLine" id="cb8-4" title="4">plt.title(<span class="st">&quot;Distribuição Valor Médio&quot;</span>)</a>
<a class="sourceLine" id="cb8-5" title="5">plt.xlabel(<span class="st">&quot;Preço&quot;</span>)</a>
<a class="sourceLine" id="cb8-6" title="6">plt.ylabel(<span class="st">&quot;Quantidade&quot;</span>)</a></code></pre></div>
<p><img src="../images/output_14_1.png"></p>
<p>Vemos que as distribuições ficaram muito mais centradas e tendendo a distribuição gaussiana<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, o que será excelente para o ajuste dos nossos estimadores<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. Sendo a função logarítmica e a função f.x+1 bijetoras, poderemos retornar ao nosso valor original assim que acabarmos o ajuste do modelo.</p>
<h4 id="simplificando-nossos-dados">Simplificando nossos dados</h4>
<p>Nossos dados ainda podem estar muito complexos, a escala em que se encontram e talvez um excesso de informação necessária podem impossibilitar que nosso modelo atinja a perfeição. Aqui iremos aplicar duas técnicas, a primeira e escalonamento de variáveis pelo máximo-mínimo, transformação que também é reversível é poderá ser desfeita ao preço final, bastando eu guardar as variáveis da minha transformação.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1">df.std()</a></code></pre></div>
<p>CRIM 1.020192 ZN 1.620831 INDUS 6.860353 CHAS 0.176055 NOX 0.115878 RM 0.702617 AGE 28.148861 DIS 0.413390 RAD 0.751839 TAX 168.537116 PTRATIO 2.164946 B 91.294864 LSTAT 0.539033 MEDV 0.386966 dtype: float64</p>
<p>É visível que algumas variáveis estão extremamente dispersas, podemos mudar isso com a seguinte formula</p>
<p><br /><span class="math display">$$ z_i=\frac{x_i-\min(x)}{\max(x)-\min(x)} $$</span><br /></p>
<p>Assim nossas variáveis estarão entre zero e um, ficando mais simplificada a predição.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1">dfmin, dfmax <span class="op">=</span> df.<span class="bu">min</span>(), df.<span class="bu">max</span>()</a>
<a class="sourceLine" id="cb10-2" title="2">df <span class="op">=</span> (df <span class="op">-</span> df.<span class="bu">min</span>())<span class="op">/</span>(df.<span class="bu">max</span>()<span class="op">-</span>df.<span class="bu">min</span>())</a>
<a class="sourceLine" id="cb10-3" title="3">df.std()</a></code></pre></div>
<p>CRIM 0.227050 ZN 0.351200 INDUS 0.251479 CHAS 0.253994 NOX 0.238431 RM 0.134627 AGE 0.289896 DIS 0.227300 RAD 0.297672 TAX 0.321636 PTRATIO 0.230313 B 0.230205 LSTAT 0.202759 MEDV 0.180819 dtype: float64</p>
<p>Excelente!!</p>
<h4 id="tudo-pronto">Tudo Pronto</h4>
<p>Finalizado nosso ajuste nos dados após tanto trabalho vamos agora para o ajuste dos nossos modelos, acostume-se, tratar os dados é o que lhe consumirá mais tempo em um processo de aprendizado de máquina. Mas por fim vamos dar uma olhada final em como eles ficaram distribuídos. Usarei a função interna do Pandas, boxplot, se tem dúvida do que esse gráfico representa, veja <a href="https://pt.wikipedia.org/wiki/Diagrama_de_caixa">aqui</a>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1">df.plot.box(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>), patch_artist<span class="op">=</span><span class="va">True</span>)</a></code></pre></div>
<p><img src="../images/output_20_1.png" width="600px"></p>
<p>Como já discutido em outras postagens, devemos separar os dados em um conjunto de treino e teste, para treinar nosso modelo e para saber quão bem nosso modelo irá prever para casos desconhecidos. Leia <a href="../2017/04/29/Um-Olhar-Descontraido-Sobre-o-Dilema-Vies-Variancia/">essa publicação</a> para entender melhor.</p>
<p>Aqui usamos a função interna do Scikit-Learn para separar os dados, para informações adicionais sobre todas as variáveis das funções abaixo sugiro consultar a <a href="http://scikit-learn.org/stable/documentation.html">documentação oficial</a>. Como primeiro argumento passo meu X, atributos, e segundo argumento meu y, valor que eu desejo prever, por fim passo um inteiro para tornar meus resultados reprodutíveis, tornando os processos aleatórios das funções não-aleatórios.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</a>
<a class="sourceLine" id="cb12-2" title="2">xtrain, xtest, ytrain, ytest <span class="op">=\</span></a>
<a class="sourceLine" id="cb12-3" title="3">    train_test_split(df.drop(<span class="st">'MEDV'</span>,<span class="dv">1</span>).values, df[<span class="st">'MEDV'</span>].values, random_state<span class="op">=</span><span class="dv">201</span>)</a></code></pre></div>
<p>Agora importaremos nossos dois modelos, o primeiro é o XGBoost, algoritmo que vem se demonstrando extremamente eficiente em competições e o Ridge famoso algoritmo regressor. Iremos avaliar nossos modelos pelo <a href="https://pt.wikipedia.org/wiki/Erro_quadr%C3%A1tico_m%C3%A9dio">erro médio quadrático</a>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" title="1"><span class="im">import</span> xgboost <span class="im">as</span> xgb</a>
<a class="sourceLine" id="cb13-2" title="2"><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</a>
<a class="sourceLine" id="cb13-3" title="3"></a>
<a class="sourceLine" id="cb13-4" title="4"><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</a></code></pre></div>
<p>Aqui executo uma pequena melhoria nos hiperparametros com o GridSearchCV para buscar a combinação dos hiperparametros que me dará uma melhor predição, em seguida ajusto meu modelo aos dados e tendo ele treinando, prevejo para dados que ele desconhece, em seguida avalio o desempenho do modelo como dito.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" title="1"><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</a>
<a class="sourceLine" id="cb14-2" title="2"></a>
<a class="sourceLine" id="cb14-3" title="3">params <span class="op">=</span> {<span class="st">'alpha'</span>: np.linspace(<span class="fl">0.1</span>,<span class="dv">1</span>,<span class="dv">200</span>),</a>
<a class="sourceLine" id="cb14-4" title="4">          <span class="st">'random_state'</span>:[<span class="dv">2020</span>]}</a>
<a class="sourceLine" id="cb14-5" title="5"></a>
<a class="sourceLine" id="cb14-6" title="6">model1 <span class="op">=</span> GridSearchCV(estimator <span class="op">=</span> Ridge(), param_grid <span class="op">=</span> params)</a>
<a class="sourceLine" id="cb14-7" title="7">model1.fit(xtrain,ytrain)</a>
<a class="sourceLine" id="cb14-8" title="8">linpred <span class="op">=</span> model1.predict(xtest)</a>
<a class="sourceLine" id="cb14-9" title="9"></a>
<a class="sourceLine" id="cb14-10" title="10">err1 <span class="op">=</span> mean_squared_error(linpred, ytest)</a>
<a class="sourceLine" id="cb14-11" title="11"><span class="bu">print</span>(err1)</a></code></pre></div>
<p>0.00736161092505</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" title="1">params <span class="op">=</span> {<span class="st">'reg_alpha'</span>: np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb15-2" title="2">          <span class="st">'gamma'</span>: np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb15-3" title="3">          <span class="st">'reg_lambda'</span>: np.linspace(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>)}</a>
<a class="sourceLine" id="cb15-4" title="4"></a>
<a class="sourceLine" id="cb15-5" title="5">model2 <span class="op">=</span> GridSearchCV(estimator <span class="op">=</span> xgb.XGBRegressor(), param_grid <span class="op">=</span> params)</a>
<a class="sourceLine" id="cb15-6" title="6">model2.fit(xtrain, ytrain)</a>
<a class="sourceLine" id="cb15-7" title="7">xgbpred <span class="op">=</span> model2.predict(xtest)</a>
<a class="sourceLine" id="cb15-8" title="8"></a>
<a class="sourceLine" id="cb15-9" title="9">err2 <span class="op">=</span> mean_squared_error(xgbpred, ytest)</a>
<a class="sourceLine" id="cb15-10" title="10"><span class="bu">print</span>(err2)</a></code></pre></div>
<p>0.00526337776169</p>
<p>Resultados muito bons, mas será que podemos deixá-los ainda melhor?! Vamos analisar se as nossas predições têm baixa correlação.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" title="1">predictions <span class="op">=</span> pd.DataFrame({<span class="st">&quot;XGBoost&quot;</span>:np.expm1(xgbpred), <span class="st">&quot;Ridge&quot;</span>:np.expm1(linpred)})</a>
<a class="sourceLine" id="cb16-2" title="2">predictions.plot(x <span class="op">=</span> <span class="st">&quot;XGBoost&quot;</span>, y <span class="op">=</span> <span class="st">&quot;Ridge&quot;</span>, kind <span class="op">=</span> <span class="st">&quot;scatter&quot;</span>, color<span class="op">=</span><span class="st">&quot;#85C8DD&quot;</span>)</a></code></pre></div>
<p><img src="../images/output_30_1.png"></p>
<p>Como já explicado, uma baixa correlação tende a melhorar significativamente nossa predição, visualmente temos algo significante, vamos olhar agora isso em números</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" title="1"><span class="im">from</span> scipy <span class="im">import</span> stats</a>
<a class="sourceLine" id="cb17-2" title="2">_, _, r_value, _, std_err <span class="op">=</span> stats.linregress(np.expm1(xgbpred),np.expm1(linpred))</a>
<a class="sourceLine" id="cb17-3" title="3"><span class="bu">print</span>(r_value, std_err)</a></code></pre></div>
<p>0.923252641379 0.0321275120299</p>
<p>Devido nosso r-valor não ser muito alto (&lt;.98), podemos nos beneficiar da combinação das estimativas. Chegamos a parte da motivação inicial combinar os modelos para aumentar o desempenho preditivo. Testarei 3 combinações das predições, média ponderada, media simples e média harmônica.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" title="1">err3 <span class="op">=</span> mean_squared_error(xgbpred <span class="op">*</span> <span class="fl">0.8</span> <span class="op">+</span> linpred <span class="op">*</span> <span class="fl">0.2</span>, ytest) <span class="co"># media ponderada</span></a>
<a class="sourceLine" id="cb18-2" title="2">err4 <span class="op">=</span> mean_squared_error((xgbpred <span class="op">+</span> linpred)<span class="op">/</span><span class="dv">2</span>, ytest) <span class="co"># media simples</span></a>
<a class="sourceLine" id="cb18-3" title="3">err5 <span class="op">=</span> mean_squared_error(stats.hmean([xgbpred, linpred]), ytest)<span class="co"># media harmonica</span></a>
<a class="sourceLine" id="cb18-4" title="4"><span class="bu">print</span>(err3, err4, err5)</a></code></pre></div>
<p>0.00499853754395 0.00524298328056 0.00517761354333</p>
<p>Excelente, ouve uma melhora significativa, mas o quão significativa?</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" title="1"><span class="dv">1</span><span class="op">-</span>err3<span class="op">/</span>err2</a></code></pre></div>
<p>0.050317539369457931</p>
<p>Está aí, 5% de melhora do nosso melhor estimador, bem significativo para algo tão simples, e tais aprimoramentos acima de algoritmos de alto desempenho são de extrema importancia no mundo da ciência de dados, talvez até nos ajudaria a pular milhares de posições rumo ao topo em uma competição valendo 1,2 milhões de dólares<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>.</p>
<h3 id="concluindo">Concluindo</h3>
<p>O objetivo principal dessa publicação era demonstrar que uma combinação simples entre dois modelos podem impactar significamente na sua predição, mas durante esse processo fiz alguns tratamentos nos dados que irão te impressionar sobre o impacto na redução do nosso erro, experimente avaliar os modelos sem realizar alguns dos tratamentos que dei aos dados… Em publicações futuras, será explicado mais sobre cada técnica vista aqui.</p>
<h4 id="referências">Referências</h4>
<p><a href="https://en.wikipedia.org/wiki/Inverse-variance_weighting">Inverse Variance</a></p>
<p><a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">Bootstrap_aggregating Wikipedia</a></p>
<p><a href="https://www.kaggle.com/apapiu/regularized-linear-models">Regularized Linear Models Kernel</a></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Polikar, R. (2006). “Ensemble based systems in decision making”. IEEE Circuits and Systems Magazine. 6 (3): 21–45. doi:10.1109/MCAS.2006.1688199<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>http://www.itl.nist.gov/div898/handbook/eda/section3/eda336.htm<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>https://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>https://stats.stackexchange.com/questions/18844/when-and-why-should-you-take-the-log-of-a-distribution-of-numbers<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>https://www.kaggle.com/c/zillow-prize-1<a href="#fnref5" class="footnote-back">↩</a></p></li>
</ol>
</section>

        </div>
        <div id="footer">
            Site generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </div>
    </body>
</html>
